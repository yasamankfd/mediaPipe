<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Face Capture</title>
    <style>
      body {
        margin: 0;
        overflow: hidden;
        font-family: Arial, sans-serif;
        background: black;
      }
      #status {
        position: absolute;
        top: 10px;
        left: 50%;
        transform: translateX(-50%);
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 10px;
        border-radius: 5px;
        font-size: 18px;
        z-index: 10;
        text-align: center;
      }
      #videoElement,
      canvas {
        position: absolute;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        object-fit: cover;
      }
      #capturedImage {
        display: none;
        position: absolute;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        object-fit: contain;
        z-index: 20;
      }
    </style>
  </head>
  <body>
    <video id="videoElement" autoplay muted playsinline></video>
    <canvas id="output"></canvas>
    <div class="status">
      <div id="status">‚è≥ Loading MediaPipe...</div>
    </div>
    <img id="capturedImage" />

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script>
      const videoElement = document.getElementById("videoElement");
      const canvasElement = document.getElementById("output");
      const canvasCtx = canvasElement.getContext("2d");
      const statusText = document.getElementById("status");
      const capturedImage = document.getElementById("capturedImage");

      let frontalTimer = 0;
      const CAPTURE_THRESHOLD = 10;
      let captured = false;
      let cameraInstance = null;
      let prevLandmarks = [];
      const STILLNESS_FRAMES = 5;
      const STILLNESS_THRESHOLD = 0.002;

      function distance(p1, p2) {
        const dx = p1.x - p2.x;
        const dy = p1.y - p2.y;
        return Math.sqrt(dx * dx + dy * dy);
      }

      function isFrontal(landmarks) {
        const leftEye = landmarks[33];
        const rightEye = landmarks[263];
        const noseTip = landmarks[1];
        const chin = landmarks[152];
        const forehead = landmarks[10];

        const midEyeX = (leftEye.x + rightEye.x) / 2;
        const horizOffset = Math.abs(noseTip.x - midEyeX);
        const eyeLevelDiff = Math.abs(leftEye.y - rightEye.y);
        const foreheadToChin = Math.abs(forehead.y - chin.y);
        const noseToChin = Math.abs(noseTip.y - chin.y);
        const pitchRatio = noseToChin / foreheadToChin;

        return (
          horizOffset < 0.01 &&
          eyeLevelDiff < 0.01 &&
          pitchRatio > 0.3 &&
          pitchRatio < 0.6
        );
      }

      function eyesAreOpen(landmarks) {
        const rightRatio =
          distance(landmarks[159], landmarks[145]) /
          distance(landmarks[133], landmarks[33]);
        const leftRatio =
          distance(landmarks[386], landmarks[374]) /
          distance(landmarks[362], landmarks[263]);
        return rightRatio > 0.15 && leftRatio > 0.15;
      }

      function isStill(current, previousSet) {
        if (previousSet.length < STILLNESS_FRAMES) return false;
        let totalDelta = 0;
        const indices = [33, 263, 1, 152, 168];

        for (let i = 0; i < previousSet.length; i++) {
          for (let j = 0; j < indices.length; j++) {
            const idx = indices[j];
            totalDelta += distance(current[idx], previousSet[i][idx]);
          }
        }
        const avgDelta = totalDelta / (indices.length * previousSet.length);
        return avgDelta < STILLNESS_THRESHOLD;
      }

      function isMouthOpen(landmarks) {
        return distance(landmarks[13], landmarks[14]) > 0.03;
      }

      function isSmiling(landmarks) {
        const leftCorner = landmarks[61];
        const rightCorner = landmarks[291];
        const upperLip = landmarks[0];
        const lowerLip = landmarks[17];

        const mouthCenterY = (upperLip.y + lowerLip.y) / 2;
        const avgCornerY = (leftCorner.y + rightCorner.y) / 2;

        const smileDelta = mouthCenterY - avgCornerY;
        return smileDelta > 0.01; // tweak as needed
      }

      function saveAndDisplayImage() {
        const imageData = canvasElement.toDataURL("image/png");
        capturedImage.src = imageData;
        capturedImage.style.display = "block";
        videoElement.style.display = "none";
        canvasElement.style.display = "none";
        statusText.textContent = "üì∏ ÿ™ÿµŸà€åÿ± ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ";
        if (cameraInstance) cameraInstance.stop();
        captured = true;
      }

      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.7,
        minTrackingConfidence: 0.7,
      });

      faceMesh.onResults((results) => {
        canvasElement.width = videoElement.videoWidth;
        canvasElement.height = videoElement.videoHeight;
        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(
          results.image,
          0,
          0,
          canvasElement.width,
          canvasElement.height
        );

        if (results.multiFaceLandmarks.length === 0) {
          statusText.textContent = "‚ùå ⁄ÜŸáÿ±Ÿá‚Äåÿß€å ÿ™ÿ¥ÿÆ€åÿµ ÿØÿßÿØŸá ŸÜÿ¥ÿØ";
          frontalTimer = 0;
          prevLandmarks = [];
        } else {
          const landmarks = results.multiFaceLandmarks[0];
          const frontal = isFrontal(landmarks);
          const eyesOpen = eyesAreOpen(landmarks);
          const still = isStill(landmarks, prevLandmarks);
          const mouthOpen = isMouthOpen(landmarks);
          const smiling = isSmiling(landmarks);

          if (prevLandmarks.length >= STILLNESS_FRAMES) prevLandmarks.shift();
          prevLandmarks.push(landmarks);

          if (!frontal) {
            statusText.textContent = "‚ö†Ô∏è ⁄ÜŸáÿ±Ÿá ÿ±Ÿàÿ®ÿ±Ÿà ŸÜ€åÿ≥ÿ™";
            frontalTimer = 0;
          } else if (!eyesOpen) {
            statusText.textContent = "üëÄ ⁄Üÿ¥ŸÖ‚ÄåŸáÿß ÿ®ÿ≥ÿ™Ÿá‚ÄåÿßŸÜÿØ";
            frontalTimer = 0;
          } else if (!still) {
            statusText.textContent = "üîÑ ŸÖŸÜÿ™ÿ∏ÿ± ÿ´ÿßÿ®ÿ™ ÿ¥ÿØŸÜ ⁄ÜŸáÿ±Ÿá...";
            frontalTimer = 0;
          } else if (smiling) {
            statusText.textContent = "üôÇ ŸÑÿ∑ŸÅÿß ⁄ÜŸáÿ±Ÿá ÿÆŸÜÿ´€å ÿ®ÿßÿ¥ÿØ (ÿ®ÿØŸàŸÜ ŸÑÿ®ÿÆŸÜÿØ)";
            frontalTimer = 0;
          } else if (mouthOpen) {
            statusText.textContent = "üôÇ ŸÑÿ∑ŸÅÿß ⁄ÜŸáÿ±Ÿá ÿÆŸÜÿ´€å ÿ®ÿßÿ¥ÿØ (ÿØŸáÿßŸÜ ÿ®ÿ≥ÿ™Ÿá)";
            frontalTimer = 0;
          } else {
            statusText.textContent = `‚úÖ ÿ¢ŸÖÿßÿØŸá (${(frontalTimer / 20).toFixed(
              1
            )}s)`;
            frontalTimer++;
            if (frontalTimer >= CAPTURE_THRESHOLD && !captured) {
              saveAndDisplayImage();
            }
          }
        }
        canvasCtx.restore();
      });

      const camera = new Camera(videoElement, {
        onFrame: async () => await faceMesh.send({ image: videoElement }),
        width: 1280,
        height: 720,
      });

      cameraInstance = camera;
      camera.start().then(() => {
        statusText.textContent = "üß† ÿØÿ± ÿ≠ÿßŸÑ ÿ™ÿ¥ÿÆ€åÿµ ⁄ÜŸáÿ±Ÿá...";
      });
    </script>
  </body>
</html>
