<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Face Capture</title>
    <style>
      body {
        margin: 0;
        overflow: hidden;
        font-family: Arial, sans-serif;
        background: black;
      }
      #status {
        position: absolute;
        top: 10px;
        left: 50%;
        transform: translateX(-50%);
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 10px;
        border-radius: 5px;
        font-size: 18px;
        z-index: 10;
        text-align: center;
      }

      #videoElement,
      canvas {
        position: absolute;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        object-fit: cover;
      }

      #capturedImage {
        display: none;
        position: absolute;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        object-fit: contain;
        z-index: 20;
      }
    </style>
  </head>
  <body>
    <video id="videoElement" autoplay muted playsinline></video>
    <canvas id="output"></canvas>
    <div class="status">
      <div id="status">‚è≥ Loading MediaPipe...</div>
    </div>
    <img id="capturedImage" />

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script>
      const videoElement = document.getElementById("videoElement");
      const canvasElement = document.getElementById("output");
      const canvasCtx = canvasElement.getContext("2d");
      const statusText = document.getElementById("status");
      const capturedImage = document.getElementById("capturedImage");

      let frontalTimer = 0;
      const CAPTURE_THRESHOLD = 5;
      let captured = false;
      let cameraInstance = null;

      let prevLandmarks = [];
      const STILLNESS_FRAMES = 5;
      const STILLNESS_THRESHOLD = 0.002;

      function isFrontal(landmarks) {
        const leftEye = landmarks[33];
        const rightEye = landmarks[263];
        const noseTip = landmarks[1];
        const chin = landmarks[152];
        const forehead = landmarks[10]; // between the eyebrows, good to detect downward tilt

        // Horizontal symmetry check
        const midEyeX = (leftEye.x + rightEye.x) / 2;
        const horizOffset = Math.abs(noseTip.x - midEyeX);

        // Eye level symmetry
        const eyeLevelDiff = Math.abs(leftEye.y - rightEye.y);

        // Pitch detection: vertical distance between forehead ‚Üí nose ‚Üí chin
        const foreheadToChin = Math.abs(forehead.y - chin.y);
        const noseToChin = Math.abs(noseTip.y - chin.y);
        const noseToForehead = Math.abs(noseTip.y - forehead.y);

        const pitchRatio = noseToChin / foreheadToChin; // Should be between ~0.35 - 0.65 for frontal faces

        // Thresholds
        const maxHorizOffset = 0.01;
        const maxEyeLevelDiff = 0.01;
        const minPitchRatio = 0.3;
        const maxPitchRatio = 0.5;

        return (
          horizOffset < maxHorizOffset &&
          eyeLevelDiff < maxEyeLevelDiff &&
          pitchRatio > minPitchRatio &&
          pitchRatio < maxPitchRatio
        );
      }

      // function distance(p1, p2) {
      //   return Math.sqrt(Math.pow(p1.x - p2.x, 2) + Math.pow(p1.y - p2.y, 2));
      // }
      function distance(p1, p2) {
        const dx = p1.x - p2.x;
        const dy = p1.y - p2.y;
        return Math.sqrt(dx * dx + dy * dy);
      }

      function getEAR(landmarks, left) {
        const indices = left
          ? [362, 385, 387, 263, 373, 380] // Right eye (mirrored)
          : [33, 160, 158, 133, 153, 144]; // Left eye
        const p1 = landmarks[indices[1]];
        const p2 = landmarks[indices[2]];
        const p3 = landmarks[indices[5]];
        const p4 = landmarks[indices[4]];
        const p0 = landmarks[indices[0]];
        const p5 = landmarks[indices[3]];

        const vertical = (distance(p1, p2) + distance(p3, p4)) / 2.0;
        const horizontal = distance(p0, p5);
        return vertical / horizontal;
      }

      function eyesAreOpen(landmarks) {
        // Right eye (horizontal: 33‚Äì133, vertical: 159‚Äì145)
        const upperRight = landmarks[159];
        const lowerRight = landmarks[145];
        const leftRight = landmarks[133];
        const rightRight = landmarks[33];

        const verticalRight = distance(upperRight, lowerRight);
        const horizontalRight = distance(leftRight, rightRight);
        const ratioRight = verticalRight / horizontalRight;

        // Left eye (horizontal: 362‚Äì263, vertical: 386‚Äì374)
        const upperLeft = landmarks[386];
        const lowerLeft = landmarks[374];
        const leftLeft = landmarks[362];
        const rightLeft = landmarks[263];

        const verticalLeft = distance(upperLeft, lowerLeft);
        const horizontalLeft = distance(leftLeft, rightLeft);
        const ratioLeft = verticalLeft / horizontalLeft;

        const eyeOpenThreshold = 0.15; // Empirically tested for normalized coordinates

        return ratioRight > eyeOpenThreshold && ratioLeft > eyeOpenThreshold;
      }

      function isStill(current, previousSet) {
        if (previousSet.length < STILLNESS_FRAMES) return false;
        let totalDelta = 0;
        const indices = [33, 263, 1, 152, 168];

        for (let i = 0; i < previousSet.length; i++) {
          for (let j = 0; j < indices.length; j++) {
            const idx = indices[j];
            totalDelta += distance(current[idx], previousSet[i][idx]);
          }
        }
        const avgDelta = totalDelta / (indices.length * previousSet.length);
        return avgDelta < STILLNESS_THRESHOLD;
      }

      function saveAndDisplayImage() {
        const imageData = canvasElement.toDataURL("image/png");
        capturedImage.src = imageData;
        capturedImage.style.display = "block";
        videoElement.style.display = "none";
        canvasElement.style.display = "none";
        statusText.textContent = "üì∏ ÿ™ÿµŸà€åÿ± ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ";
        if (cameraInstance) {
          cameraInstance.stop();
        }
        captured = true;
      }

      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.7,
        minTrackingConfidence: 0.7,
      });

      faceMesh.onResults((results) => {
        canvasElement.width = videoElement.videoWidth;
        canvasElement.height = videoElement.videoHeight;

        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(
          results.image,
          0,
          0,
          canvasElement.width,
          canvasElement.height
        );

        if (results.multiFaceLandmarks.length === 0) {
          statusText.textContent = "‚ùå ⁄ÜŸáÿ±Ÿá‚Äåÿß€å ÿ™ÿ¥ÿÆ€åÿµ ÿØÿßÿØŸá ŸÜÿ¥ÿØ";
          frontalTimer = 0;
          prevLandmarks = [];
        } else {
          const landmarks = results.multiFaceLandmarks[0];
          const frontal = isFrontal(landmarks);
          const eyesOpen = eyesAreOpen(landmarks);
          const still = isStill(landmarks, prevLandmarks);

          if (prevLandmarks.length >= STILLNESS_FRAMES) {
            prevLandmarks.shift();
          }
          prevLandmarks.push(landmarks);

          if (!frontal) {
            statusText.textContent = "‚ö†Ô∏è ⁄ÜŸáÿ±Ÿá ÿ±Ÿàÿ®ÿ±Ÿà ŸÜ€åÿ≥ÿ™";
            frontalTimer = 0;
          } else if (!eyesOpen) {
            statusText.textContent = "üëÄ ⁄Üÿ¥ŸÖ‚ÄåŸáÿß ÿ®ÿ≥ÿ™Ÿá‚ÄåÿßŸÜÿØ";
            frontalTimer = 0;
          } else if (!still) {
            statusText.textContent = "üîÑ ŸÖŸÜÿ™ÿ∏ÿ± ÿ´ÿßÿ®ÿ™ ÿ¥ÿØŸÜ ⁄ÜŸáÿ±Ÿá...";
            frontalTimer = 0;
          } else {
            statusText.textContent = `‚úÖ ÿ¢ŸÖÿßÿØŸá (${(frontalTimer / 20).toFixed(
              1
            )}s)`;
            frontalTimer++;
            if (frontalTimer >= CAPTURE_THRESHOLD && !captured) {
              saveAndDisplayImage();
            }
          }
        }

        canvasCtx.restore();
      });

      const camera = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 1280,
        height: 720,
      });

      cameraInstance = camera;

      camera.start().then(() => {
        statusText.textContent = "üß† ÿØÿ± ÿ≠ÿßŸÑ ÿ™ÿ¥ÿÆ€åÿµ ⁄ÜŸáÿ±Ÿá...";
      });
    </script>
  </body>
</html>
